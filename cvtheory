
---

# ğŸ§© **PRACTICAL 1 â€“ Basic Image Handling and Processing**

### ğŸ¯ **Aim**

To perform basic image operations such as grayscale conversion, blurring, and edge detection using OpenCV.

### ğŸ“˜ **Theory**

* **Image** = a matrix of pixel values.
* **Color image**: 3 channels (B, G, R).
* **Grayscale**: 1 channel (intensity only).
* **Blurring**: Smooths the image, reduces noise using a Gaussian filter.
* **Edge detection**: Identifies strong intensity changes using gradient operators.

### ğŸ§  **Important Full Forms**

* **BGR** â€“ Blue, Green, Red
* **RGB** â€“ Red, Green, Blue
* **Canny** â€“ Named after John F. Canny (developer of the Canny edge detection algorithm).

### ğŸ’¡ **Code Meaning**

* `cv2.imread()` â†’ Loads an image (we use `np.zeros` to create one).
* `cv2.cvtColor()` â†’ Converts color format (BGRâ†’GRAY).
* `cv2.GaussianBlur()` â†’ Applies Gaussian smoothing.
* `cv2.Canny()` â†’ Detects edges by thresholding gradients.
* `plt.imshow()` â†’ Displays the image with Matplotlib.

### ğŸ§© **Example**

A black image with a white rectangle and circle â†’ converted to gray â†’ blurred â†’ edges extracted.

### â“ **Viva Q&A**

* **Q:** Why convert to grayscale?
  **A:** Many algorithms (edge/corner) depend only on intensity, not color.
* **Q:** What does Gaussian blur do?
  **A:** Removes high-frequency noise by averaging pixels.
* **Q:** What are edges?
  **A:** Boundaries between regions of different intensity.

---

# ğŸ§© **PRACTICAL 2 â€“ Geometric Transformations**

### ğŸ¯ **Aim**

To perform geometric operations like translation, rotation, reflection, and shearing.

### ğŸ“˜ **Theory**

Geometric transformation modifies image position, size, or orientation using matrices.
They are **Affine Transformations** (preserve straight lines and parallelism).

### ğŸ“— **Full Forms**

* **ROI** â€“ Region of Interest
* **Affine** â€“ Linear + Translation (no distortion)

### ğŸ§  **Transformations**

| Operation       | Meaning                      | Example           |
| --------------- | ---------------------------- | ----------------- |
| **Translation** | Moves image                  | Shift by (tx, ty) |
| **Rotation**    | Rotates image around a point | 45Â° rotation      |
| **Reflection**  | Mirror image                 | Horizontal flip   |
| **Shearing**    | Slants the image             | x' = x + kÂ·y      |

### ğŸ’¡ **Code Meaning**

* `cv2.warpAffine()` â†’ Applies a 2Ã—3 affine matrix.
* `cv2.getRotationMatrix2D()` â†’ Returns rotation + translation matrix.
* `np.float32([[1,0,tx],[0,1,ty]])` â†’ Translation matrix.
* `[-1,0,w]` â†’ Reflect across Y-axis.

### â“ **Viva Q&A**

* **Q:** What is affine transformation?
  **A:** A linear mapping followed by translation that preserves lines and parallelism.
* **Q:** Why use `warpAffine`?
  **A:** It applies 2D transformation using matrix operations.
* **Q:** Example of real use?
  **A:** Rotating scanned documents, flipping selfies, object alignment.

---

# ğŸ§© **PRACTICAL 3 â€“ Homography Matrix**

### ğŸ¯ **Aim**

To compute and apply a homography matrix to map one image plane to another.

### ğŸ“˜ **Theory**

* **Homography** is a 3Ã—3 matrix used for **projective transformations**.
* It corrects perspective distortions.
* Requires **minimum 4 pairs** of corresponding points.

### ğŸ“— **Full Forms**

* **DLT** â€“ Direct Linear Transform (algorithm used)
* **RANSAC** â€“ Random Sample Consensus (for robust estimation)

### ğŸ’¡ **Formula**

[
s[x', y', 1]^T = H[x, y, 1]^T
]
where `H` is the 3Ã—3 homography matrix.

### ğŸ’¡ **Code Meaning**

* `cv2.findHomography(src, dst)` â†’ Calculates H from 4+ points.
* `cv2.warpPerspective(img, H, size)` â†’ Applies the transformation.

### ğŸ§  **Example**

If a rectangular paper looks tilted in an image, homography can flatten it to look as if taken from above.

### â“ **Viva Q&A**

* **Q:** What does homography do?
  **A:** Maps points between two views of the same plane (used for perspective correction).
* **Q:** How many points needed?
  **A:** At least 4 corresponding pairs.
* **Q:** Applications?
  **A:** Document scanning, panorama stitching.

---

# ğŸ§© **PRACTICAL 4 â€“ Perspective Transformation**

### ğŸ¯ **Aim**

To apply a perspective transformation to correct viewing angle or perform projection.

### ğŸ“˜ **Theory**

* Perspective transform is a **special case of homography**.
* Straight lines remain straight, but parallel lines may converge.

### ğŸ’¡ **Formula**

[
x' = \frac{a_1x + b_1y + c_1}{g_1x + h_1y + 1}, \quad
y' = \frac{a_2x + b_2y + c_2}{g_1x + h_1y + 1}
]

### ğŸ’¡ **Code Meaning**

* `cv2.getPerspectiveTransform(src, dst)` â†’ Gets 3Ã—3 perspective matrix.
* `cv2.warpPerspective()` â†’ Applies it to the image.

### ğŸ§  **Example**

Capturing a tilted ID card â†’ applying perspective transform to make it rectangular.

### â“ **Viva Q&A**

* **Q:** Difference between affine and perspective transform?
  **A:** Affine preserves parallel lines; perspective doesnâ€™t.
* **Q:** Application?
  **A:** Used in augmented reality, document scanners.

---

# ğŸ§© **PRACTICAL 5 â€“ Camera Calibration**

### ğŸ¯ **Aim**

To estimate cameraâ€™s internal and external parameters using known 3Dâ€“2D correspondences.

### ğŸ“˜ **Theory**

* Relates **3D world points** â†’ **2D image points**.
* Determines:

  * **Intrinsic parameters (K)** â†’ focal length, principal point.
  * **Extrinsic parameters** â†’ rotation (R), translation (t).
  * **Distortion coefficients** â†’ correct lens distortion.

### ğŸ“— **Full Forms**

* **RMS** â€“ Root Mean Square (reprojection error)
* **FOV** â€“ Field of View
* **Pinhole camera model** â€“ Simple geometric model used in calibration.

### ğŸ’¡ **Equation**

[
s \begin{bmatrix} u \ v \ 1 \end{bmatrix} = K [R | t] \begin{bmatrix} X \ Y \ Z \ 1 \end{bmatrix}
]

### ğŸ’¡ **Code Meaning**

* `objp` â†’ Known 3D coordinates (e.g., chessboard).
* `imgp` â†’ Corresponding 2D image points.
* `cv2.calibrateCamera()` â†’ Solves for K, distortion, R, t.

### â“ **Viva Q&A**

* **Q:** What is intrinsic parameter?
  **A:** Parameters internal to the camera (focal length, optical center).
* **Q:** Why calibrate?
  **A:** To correct distortions and perform accurate 3D measurements.

---

# ğŸ§© **PRACTICAL 6 â€“ Fundamental Matrix**

### ğŸ¯ **Aim**

To compute the fundamental matrix between two stereo images.

### ğŸ“˜ **Theory**

* Describes **epipolar geometry** between two images.
* Equation:
  [
  x'^T F x = 0
  ]
* `F` relates points between two images such that point x in left image corresponds to an epipolar line in right image.

### ğŸ“— **Full Forms**

* **E** â€“ Essential Matrix (calibrated version of F)
* **F** â€“ Fundamental Matrix
* **FM_8POINT** â€“ 8-point algorithm method in OpenCV

### ğŸ’¡ **Code Meaning**

* `cv2.findFundamentalMat(pts1, pts2, method)` â†’ computes F.
* `FM_8POINT` â†’ classic algorithm using 8 correspondences.

### â“ **Viva Q&A**

* **Q:** What does F represent?
  **A:** It encodes the relation between two views of the same scene.
* **Q:** What is the difference between F and E?
  **A:** F works for uncalibrated cameras; E is for calibrated ones.
* **Q:** Use?
  **A:** Used in stereo vision and 3D reconstruction.

---

# ğŸ§© **PRACTICAL 7 â€“ Edge, Line, and Corner Detection**

### ğŸ¯ **Aim**

To detect edges, lines, and corners using different algorithms.

### ğŸ“˜ **Theory**

* **Edges:** Sudden intensity changes. Detected by **Canny**.
* **Lines:** Detected using **Hough Transform** (maps points to sinusoids).
* **Corners:** Detected using **Harris** method (variation in both x and y).

### ğŸ“— **Full Forms**

* **HOG** â€“ Histogram of Oriented Gradients
* **ROI** â€“ Region of Interest
* **Canny** â€“ Edge detector algorithm by John Canny

### ğŸ’¡ **Code Meaning**

* `cv2.Canny()` â†’ Detects edges.
* `cv2.HoughLinesP()` â†’ Finds straight lines.
* `cv2.cornerHarris()` â†’ Finds corners by analyzing intensity variations.

### ğŸ§  **Example**

Image with rectangles and diagonals â†’ edges (boundaries), lines (green), corners (red dots).

### â“ **Viva Q&A**

* **Q:** What is a corner?
  **A:** A point with high change in intensity in both directions.
* **Q:** How is Hough Transform used?
  **A:** Detects straight lines by converting to polar coordinates.

---

# ğŸ§© **PRACTICAL 8 â€“ SIFT Feature Descriptor**

### ğŸ¯ **Aim**

To detect and describe keypoints that are scale and rotation invariant.

### ğŸ“˜ **Theory**

* **SIFT (Scale-Invariant Feature Transform)** detects points that remain the same under scaling, rotation, and lighting changes.
* Descriptors are **128-dimensional** feature vectors.

### ğŸ“— **Full Forms**

* **DOG** â€“ Difference of Gaussian
* **SIFT** â€“ Scale-Invariant Feature Transform

### ğŸ’¡ **Main Steps**

1. Scale-space extrema detection
2. Keypoint localization
3. Orientation assignment
4. Descriptor computation (gradient histograms)

### ğŸ’¡ **Code Meaning**

* `cv2.SIFT_create()` â†’ Creates SIFT object.
* `detectAndCompute()` â†’ Finds keypoints and their descriptors.
* `drawKeypoints()` â†’ Visualizes keypoints.

### â“ **Viva Q&A**

* **Q:** Why SIFT is invariant?
  **A:** Because it normalizes for scale, rotation, and brightness.
* **Q:** Application?
  **A:** Image matching, object recognition, panorama stitching.

---

# ğŸ§© **PRACTICAL 9 â€“ SURF and HOG**

### ğŸ¯ **Aim**

To implement SURF (fast robust features) and HOG (gradient-based descriptor).

### ğŸ“˜ **Theory**

* **SURF (Speeded-Up Robust Features)**: Faster version of SIFT using **Hessian matrix** and **box filters**.
* **HOG (Histogram of Oriented Gradients)**: Describes object shape using local edge directions.

### ğŸ“— **Full Forms**

* **SURF** â€“ Speeded-Up Robust Features
* **HOG** â€“ Histogram of Oriented Gradients
* **ROI** â€“ Region of Interest

### ğŸ’¡ **Code Meaning**

* `cv2.xfeatures2d.SURF_create(400)` â†’ Creates SURF detector.
* `detectAndCompute()` â†’ Gets feature keypoints and descriptors.
* `cv2.HOGDescriptor()` â†’ Computes HOG feature vector for an image.

### ğŸ§  **Applications**

| Method   | Used For                             |
| -------- | ------------------------------------ |
| **SURF** | Object recognition, feature matching |
| **HOG**  | Human/vehicle detection              |

### â“ **Viva Q&A**

* **Q:** Difference between SIFT and SURF?
  **A:** SURF is faster, uses integral images.
* **Q:** What does HOG capture?
  **A:** The distribution of edge orientations (shape info).
* **Q:** Why use descriptors?
  **A:** They convert image patches into numerical features for matching.

---

# ğŸ’¯ **FINAL VIVA QUICK REVISION TABLE**

| Practical | Topic              | Key Concept                  | Real-Life Use          |
| --------- | ------------------ | ---------------------------- | ---------------------- |
| 1         | Image Basics       | Grayscale, blur, edge        | Preprocessing          |
| 2         | Geometric          | Rotate, translate, reflect   | Image alignment        |
| 3         | Homography         | Plane-to-plane mapping       | Perspective correction |
| 4         | Perspective        | Rectify viewpoint            | Document scanning      |
| 5         | Calibration        | Camera intrinsics/extrinsics | 3D measurement         |
| 6         | Fundamental Matrix | Epipolar geometry            | Stereo vision          |
| 7         | Edge/Line/Corner   | Feature extraction           | Shape detection        |
| 8         | SIFT               | Scale-invariant features     | Image matching         |
| 9         | SURF & HOG         | Robust/gradient descriptors  | Object detection       |

---
